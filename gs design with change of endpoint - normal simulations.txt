# Software (c) 2025, The University of Warwick (the "Software"), comprising of code written in R. 
#
# The Software remains the property of the University of Warwick ("the University"). 
# 
# The Software is distributed "AS IS" under this Licence solely for non-commercial use, 
# such as academic research, clinical research, and clinical trials, in the hope that it 
# will be useful, but in order that the University as a charitable foundation protects 
# its assets for the benefit of its educational and research purposes, the University  
# makes clear that no condition is made or to be implied, nor is any warranty given or to  
# be implied, as to the accuracy of the Software, or that it will be suitable for any  
# particular purpose or for use under any specific conditions. Furthermore, the University  
# disclaims all responsibility for the use which is made of the Software. It further  
# disclaims any liability for the outcomes arising from using the Software. 
# 
# The Licensee agrees to indemnify the University and hold the University harmless from  
# and against any and all claims, damages and liabilities asserted by third parties  
# (including claims for negligence) which arise directly or indirectly from the use of  
# the Software. No part of the Software may be reproduced, modified, transmitted or  
# transferred in any form or by any means, electronic or mechanical, without the  
# express permission of the University. The permission of the University is not required  
# if the said reproduction, modification, transmission or transference is done without  
# financial return, the conditions of this Licence are imposed upon the receiver of the  
# product, and all original and amended source code is included in any transmitted product.  
# You may be held legally responsible for any copyright infringement that is caused or  
# encouraged by your failure to abide by these terms and conditions.
# 
# You are not permitted under this Licence to use this Software commercially. Use for which  
# any financial return is received shall be defined as commercial use, and includes  
# (1) integration of all or part of the source code or the Software into a product for sale  
# or license by or on behalf of Licensee to third parties or (2) use of the Software or any  
# derivative of it for research with the final aim of developing software products for  
# sale or license to a third party or (3) use of the Software or any derivative of it for  
# research with the final aim of developing non-software products for sale or license to  
# a third party, or (4) use of the Software to provide any service to an external organisation  
# for which payment is received.   
# 
# If you are interested in using the Software commercially, please contact Warwick Ventures  
# Limited ("WVL"), the technology transfer company of the University, to negotiate a licence.  
# Contact details are: ventures@warwick.ac.uk.  
#
# If you are in any doubt if your use constitutes commercial use, contact WVL.


library(mvtnorm)
library(survival)
library(parallel)

get.increments <- function(v){c(v[1], diff(v))
}


##################################################################################################
# Code to find group-sequential boundary for single endpoint
##################################################################################################

get.p.cross.upper.look <- function(look, I, upper, lower, mean, Sigma)		# gets probability stopping on upper boundary at this look
{
  if (look == 1) {
    pr = 1 - pnorm(upper[1], mean = mean[1], sd = sqrt(Sigma[1,1]))
  }
  else {
    u = c(upper[1:(look-1)],Inf)
    l = c(lower[1:(look-1)],upper[look])
    pr = pmvnorm(lower = l, upper = u, mean = mean[1:look], sigma = Sigma[1:look,1:look], abseps=0.00001, maxpts=10000)[1]
  }
pr
}

get.p.cross.lower.look <- function(look, I, upper, lower, mean, Sigma)		# gets probability stopping on lower boundary at this look
{
  if (look == 1) {
    pr = pnorm(lower[1], mean = mean[1], sd = sqrt(Sigma[1,1]))
  }
  else {
    u = c(upper[1:(look-1)],lower[look])
    l = c(lower[1:(look-1)],-Inf)
    pr = pmvnorm(lower = l, upper = u, mean = mean[1:look], sigma = Sigma[1:look,1:look], abseps=0.00001, maxpts=10000)[1]
  }
pr
}

get.p.cross.upper <- function(I, upper, lower, theta)		# gets probability of crossing upper boundary
{  
n.looks = length(I)
s_mean = theta*I
s_var = matrix(rep(NA, n.looks^2), ncol=n.looks)
for (i in seq(1, n.looks))
  for (j in seq(1,i))
    s_var[i,j] = s_var[j,i] = I[j]

pr_look = rep(0,n.looks)
for (look in seq(1,n.looks)) pr_look[look] = get.p.cross.upper.look(look, I, upper, lower, s_mean, s_var)
  
sum(pr_look)
}


get.p.cross.lower <- function(I, upper, lower, theta)		# gets probability of crossing lower boundary
{  
n.looks = length(I)
s_mean = theta*I
s_var = matrix(rep(NA, n.looks^2), ncol=n.looks)
for (i in seq(1, n.looks))
  for (j in seq(1,i))
    s_var[i,j] = s_var[j,i] = I[j]

pr_look = rep(0,n.looks)
for (look in seq(1,n.looks)) pr_look[look] = get.p.cross.lower.look(look, I, upper, lower, s_mean, s_var)
  
sum(pr_look)
}


upper.boundaries.search.fn <- function(upper.value, look, I, upper, lower, target)
{
  I = I[1:look]
  lower = lower[1:look]
  upper = upper[1:look]
  upper[look] = upper.value
  get.p.cross.upper(I, upper, lower, theta=0) - target
}


lower.boundaries.search.fn <- function(lower.value, look, I, upper, lower, target)
{
  I = I[1:look]
  upper = upper[1:look]
  lower = lower[1:look]
  lower[look] = lower.value
  get.p.cross.lower(I, upper, lower, theta=0) - target
}


get.boundary.fixed.I <- function(I, alpha.star.u, alpha.star.l)
{
  n.looks = length(I)
  alpha.star.u.inc = get.increments(alpha.star.u)
  alpha.star.l.inc = get.increments(alpha.star.l)
  
  upper = rep(Inf, n.looks)
  lower = rep(-Inf, n.looks)
  for (look in seq(1, n.looks))
  {
    if (alpha.star.u.inc[look] > 0) 
      upper[look] = uniroot(upper.boundaries.search.fn, c(-10,10)*sqrt(I[look]), look, I, upper, lower, target=alpha.star.u[look])$root

    if (alpha.star.l.inc[look] > 0) 
      lower[look] = uniroot(lower.boundaries.search.fn, c(-10,10)*sqrt(I[look]), look, I, upper, lower, target=alpha.star.l[look])$root
  }
  data.frame(I, upper, lower, upper.z = upper/sqrt(I), lower.z = lower/sqrt(I))
}


power.search.fn <- function(I.max,boundary.unscaled,theta,target)
{
difference = rep(NA,length(I.max))
for (i in seq(1,length(I.max)))
	{
	I = I.max[i]*boundary.unscaled$I
	upper = boundary.unscaled$upper*sqrt(I.max/max(boundary.unscaled$I))
	lower = boundary.unscaled$lower*sqrt(I.max/max(boundary.unscaled$I))
	difference[i] = get.p.cross.upper(I, upper, lower, theta) - target
	}
difference
}


get.boundary <- function(t, alpha.star.u, alpha.star.l, theta, power)
{
boundary.unscaled = get.boundary.fixed.I(t, alpha.star.u, alpha.star.l)

I.max = uniroot(power.search.fn, c(1,1000), boundary.unscaled=boundary.unscaled, theta=theta, target=power)$root

scale.factor = I.max/max(boundary.unscaled$I)
data.frame(I = boundary.unscaled$I*scale.factor, upper = boundary.unscaled$upper*sqrt(scale.factor), lower = boundary.unscaled$lower*sqrt(scale.factor), upper.z = boundary.unscaled$upper.z, lower.z=boundary.unscaled$lower.z)
}




##################################################################################################
# Code to find critical values after change of endpoint
##################################################################################################


get.error.prob <- function(look, k.tilde, I.A, I.B, theta.A, rho, upper.A, upper.B)
{

K = length(I.A)	# assume length(I.A) = length(I.B)
lower.A = rep(-Inf,K)
lower.B = rep(-Inf,K)

# get mean and variance-covariance matrix of (S.A, S.B)
mean.AB = c(theta.A*I.A,0*I.B)
var.AB = matrix(rep(NA,(2*K)^2),ncol=2*K)
for (i in seq(1,K)) 
	for (j in seq(1,i)) 
		{
		var.AB[i,j] = var.AB[j,i] = I.A[j]
		var.AB[K+i,K+j] = var.AB[K+j,K+i] = I.B[j]
		var.AB[K+i,j] = var.AB[K+j,i] = var.AB[i,K+j] = var.AB[j,K+i] = rho*sqrt(I.A[j]*I.B[j])
		}

# get probability of rejecting H_0B at this look in different cases depending on whether we are before, at or after k.tilde
if (look < k.tilde)
	{
	if (look==1) 
		{
		A.i.stop = 1		# give boundaries for S.A[1] and S.B[1] only
		B.i.stop = 1
		lower = c(upper.A[A.i.stop],upper.B[B.i.stop])
		upper = c(Inf,Inf)
		mean = mean.AB[c(A.i.stop,K+B.i.stop)]
		Sigma = var.AB[c(A.i.stop,K+B.i.stop),c(A.i.stop,K+B.i.stop)]
		prob = pmvnorm(lower,upper,mean=mean,sigma=Sigma, abseps=0.00001, maxpts=10000)[1]
		}
	if ((look>1)*(look<K)==1)
 		{
		A.i.cont = seq(1,look-1)		# give boundaries for S.A[1],S.A[2],...,S.A[look] and S.B[look] only
		A.i.stop = look
		B.i.stop = look
		lower = c(lower.A[A.i.cont],upper.A[A.i.stop],upper.B[B.i.stop]) 
		upper = c(upper.A[A.i.cont],Inf,Inf)
		mean = mean.AB[c(A.i.cont,A.i.stop,K+B.i.stop)]
		Sigma = var.AB[c(A.i.cont,A.i.stop,K+B.i.stop),c(A.i.cont,A.i.stop,K+B.i.stop)]
		prob = pmvnorm(lower,upper,mean=mean,sigma=Sigma, abseps=0.00001, maxpts=10000)[1]
		}
	if (look==K)
 		{
		A.i.cont = seq(1,look-1)		# give boundaries for S.A[1],S.A[2],...,S.A[look] and S.B[look] only
		B.i.stop = look
		lower = c(lower.A[A.i.cont],upper.B[B.i.stop]) 
		upper = c(upper.A[A.i.cont],Inf)
		mean = mean.AB[c(A.i.cont,K+B.i.stop)]
		Sigma = var.AB[c(A.i.cont,K+B.i.stop),c(A.i.cont,K+B.i.stop)]
		prob = pmvnorm(lower,upper,mean=mean,sigma=Sigma, abseps=0.00001, maxpts=10000)[1]
		}
	}

if (look == k.tilde)
	{
	A.i.cont = seq(1,look-1)		# give boundaries for S.A[1],S.A[2],...,S.A[look] and S.B[look] only
	B.i.stop = look
	lower = c(lower.A[A.i.cont],upper.B[B.i.stop]) 
	upper = c(upper.A[A.i.cont],Inf)
	mean = mean.AB[c(A.i.cont,K+B.i.stop)]
	Sigma = var.AB[c(A.i.cont,K+B.i.stop),c(A.i.cont,K+B.i.stop)]
	prob = pmvnorm(lower,upper,mean=mean,sigma=Sigma, abseps=0.00001, maxpts=10000)[1]
	}

if (look > k.tilde)
	{
	A.i.cont = seq(1,k.tilde-1)		# give boundaries for S.A[1],S.A[2],...,S.A[look] and S.B[look] only
	B.i.cont = seq(k.tilde,look-1)
	B.i.stop = look
	lower = c(lower.A[A.i.cont],lower.B[B.i.cont],upper.B[B.i.stop]) 
	upper = c(upper.A[A.i.cont],upper.B[B.i.cont],Inf)
	mean = mean.AB[c(A.i.cont,K+B.i.cont,K+B.i.stop)]
	Sigma = var.AB[c(A.i.cont,K+B.i.cont,K+B.i.stop),c(A.i.cont,K+B.i.cont,K+B.i.stop)]
	prob = pmvnorm(lower,upper,mean=mean,sigma=Sigma, abseps=0.00001, maxpts=10000)[1]
	}

prob
}


max.error.fn <- function(theta.A, look, k.tilde, I.A, I.B, rho, upper.A, upper.B)
{
answer = rep(NA,length(theta.A))
for (i in seq(1,length(theta.A))) 
	{
	answer[i] = 0 
	for (k in seq(1,look)) answer[i]=answer[i] + get.error.prob(k, k.tilde, I.A, I.B, theta.A[i], rho, upper.A, upper.B)
	}
answer
}

#	if (k.tilde==2) if (look==2) 
#	{
#	rho2=rho*sqrt(I.B[1]/I.B[2])
#	num1 = (upper.B[2]/sqrt(I.B[2]) - rho2*upper.A[1]/sqrt(I.A[1])) / sqrt(1-rho2^2)
#	num2 = (upper.B[1]/sqrt(I.B[1]) - rho*upper.A[1]/sqrt(I.A[1])) / sqrt(1-rho^2)
#	den = sqrt(I.A[1]) * ( rho/sqrt(1-rho^2) - rho2/sqrt(1-rho2^2) )
#	theta.A.star = (num1 - num2)/den
#	}


get.max.error.search <- function(upper.B.look, look, k.tilde, I.A, I.B, rho, upper.A, upper.B, target, return.theta.A=F)	# first argument is upper.B[look] for searching
{
difference = rep(NA,length(upper.B.look))
theta.A.star = rep(NA,length(upper.B.look))

for (i in seq(1,length(upper.B.look)))
{
upper.B[look] = upper.B.look[i]

if (look==1) 
	{
	theta.A.star[i] = 100
	max.error = max.error.fn(theta.A.star, look, k.tilde, I.A, I.B, rho, upper.A, upper.B)		# theta.A = infinity
	}
else 
	{
	optimise.output  = optimise(max.error.fn,interval=c(-3,3),look, k.tilde, I.A=I.A, I.B=I.B, rho=rho, upper.A=upper.A, upper.B=upper.B, maximum=T)
	theta.A.star[i] = optimise.output$maximum
	max.error = optimise.output$objective
	}

difference[i] = max.error - target
}

if (return.theta.A) output=data.frame(difference,theta.A.star)
else output=difference
output
}




##################################################################################################
# Simulate trials with normal data
##################################################################################################

get.x.A.B <- function(n0,n1,theta.A=0,theta.B=0)
{
# simulate correlated endpoints
# note that scaling does not change S or I.  Hence set sd to be 1 for both endpoints
mean.A.0 = 0
mean.A.1 = theta.A
mean.B.0 = 0
mean.B.1 = theta.B

sd.A.0 = sd.A.1 = 1
sd.B.0 = sd.B.1 = 1
rho = 0.7

var.mat.0 = matrix(c(sd.A.0^2,rho*sd.A.0*sd.B.0,rho*sd.A.0*sd.B.0,sd.B.0^2),ncol=2)
x.AB.0 = rmvnorm(n0,mean=c(mean.A.0,mean.B.0),sigma=var.mat.0)

var.mat.1 = matrix(c(sd.A.1^2,rho*sd.A.1*sd.B.1,rho*sd.A.1*sd.B.1,sd.B.1^2),ncol=2)
x.AB.1 = rmvnorm(n1,mean=c(mean.A.1,mean.B.1),sigma=var.mat.1)

x.AB = rbind(x.AB.0,x.AB.1)
treatment = c(rep(0,n0),rep(1,n1))
x = data.frame(x.A=x.AB[,1],x.B=x.AB[,2],treatment)
x
}


get.S.I.normal <- function(x,treatment)
{
n = length(x)
n1 = sum(treatment)
n0 = n-n1

sum0 = sum(x[treatment==0])
sum1 = sum(x[treatment==1])
D = sd(x) * sqrt((n-1)/n)

S = (n0*sum1 - n1*sum0)/(n * D)
I = n0*n1/n - S^2/(2*n)

data.frame(S,I)
}


get.cor <- function(x.A,x.B,treatment)
{
n = length(x.A)
n1 = sum(treatment)
n0 = n-n1

( n0*cor(x.A[treatment==0],x.B[treatment==0]) + n1*cor(x.A[treatment==1],x.B[treatment==1]) ) / n
}



simulate.trial <- function(sim=1, t, I.max, theta.A, theta.B, k.tilde, print=0, seed=NA)
{
# sim is dummy argument to allow function to be called by mcapply
if (!is.na(seed)) set.seed(seed+sim)	# for reproducible random numbers

n.total = 2*I.max			# required sample size per group in total
n = round( t*n.total)			# required sample size per group at each look
n.inc = get.increments(n)

# simulate data

S.A = I.A = rep(NA,length(t))
S.B = I.B = rep(NA,length(t))

for (look in seq(1,K))
{
if (look==1) x.A = x.B = treatment = NULL

x.inc = get.x.A.B(n.inc[look],n.inc[look],theta.A=theta.A,theta.B=theta.B)
x.A.inc = x.inc$x.A
x.B.inc = x.inc$x.B
treatment.inc = x.inc$treatment

treatment = c(treatment,treatment.inc)
x.A = c(x.A,x.A.inc)
x.B = c(x.B,x.B.inc)

S.I.A = get.S.I.normal(x.A,treatment)
S.A[look] = S.I.A$S
I.A[look] = S.I.A$I

S.I.B = get.S.I.normal(x.B,treatment)
S.B[look] = S.I.B$S
I.B[look] = S.I.B$I
}

if (print==1) print(S.A)
if (print==1) print(S.B)


# get boundaries

stop.A = 0
stop.time = 0
for (look in seq(1,k.tilde-1))
{
if (stop.A ==0)
	{
	alpha.star.u[look] = 0.025*I.A[look]/I.max
	if (alpha.star.u[look] > 0.025) alpha.star.u[look]=0.025

	boundary = get.boundary.fixed.I(I.A[1:look],alpha.star.u[1:look],alpha.star.l=rep(0,look))
	if (print==1) print(boundary)

	upper.A = boundary$upper
	lower.A = boundary$lower
	if (S.A[look] >= upper.A[look]) 
		{
		stop.A =1
		stop.time=look
		}
	}
}

rho = upper.B = theta.A.star = check.diff = upper.B.naive = upper.B.gs = rep(NA,K)

I.B.max = I.max		# same for normal data as theta and S are scaled by sd

alpha.star.u = 0.025*I.B/I.B.max

stop = 0
rej.B = 0
stop.time.naive = stop.time
stop.naive = 0
rej.B.naive = 0
stop.time.gs = stop.time
stop.gs = 0
rej.B.gs = 0

for (look in seq(1,K))
{
# using corrected boundary
if (stop==0)
	{
	# estimate rho using all available data from stop.time or k.tilde as this is first time S.B is used
	if (stop.time == 0) 
		{
		if (look == 1) rho[look] = get.cor(x.A[seq(1,2*n[k.tilde])],x.B[seq(1,2*n[k.tilde])],treatment[seq(1,2*n[k.tilde])])
		if (look <= k.tilde) rho[look]=rho[1]
		if (look > k.tilde) rho[look] = get.cor(x.A[seq(1,2*n[look])],x.B[seq(1,2*n[look])],treatment[seq(1,2*n[look])])
		}
	if (stop.time > 0)
		{
		if (look == 1) rho[look] = get.cor(x.A[seq(1,2*n[stop.time])],x.B[seq(1,2*n[stop.time])],treatment[seq(1,2*n[stop.time])])
		rho[look]=rho[1]
		}

	target = alpha.star.u[look]

	check.diff[look]=1
	while (check.diff[look] > 0.00001)
	{
	upper.B[look] = uniroot(get.max.error.search,c(0,4)*sqrt(I.B[look]), look=look, k.tilde=k.tilde, I.A=I.A, I.B=I.B, rho=rho[look], upper.A=upper.A, upper.B=upper.B, 	target=target)$root
	search.output = get.max.error.search(upper.B[look], look=look, k.tilde=k.tilde, I.A=I.A, I.B=I.B, rho=rho[look], upper.A=upper.A, upper.B=upper.B, target=target, return.theta.A=T)
	check.diff[look] = search.output$difference
	theta.A.star[look] = search.output$theta.A.star
	}

	if (look == stop.time) 
		{
		stop=1
		if (S.B[look] > upper.B[look]) rej.B=1 
		}
	if (look >= k.tilde) if (S.B[look] > upper.B[look]) 
		{
		stop=1
		rej.B=1 
		stop.time = look
		}
	}

# using naive test
if (stop.naive==0)
	{
	upper.B.naive[look] = 1.96*sqrt(I.B[look])

	if (look == stop.time.naive) 
		{
		stop.naive=1
		if (S.B[look] > upper.B.naive[look]) rej.B.naive=1 
		}
	if (look >= k.tilde) if (S.B[look] > upper.B.naive[look]) 
		{
		stop.naive=1
		rej.B.naive=1 
		stop.time.naive = look
		}
	}

if (stop.gs==0)
	{
	upper.B.gs[look] = get.boundary.fixed.I(I.B[1:look],alpha.star.u[1:look],alpha.star.l=rep(0,look))$upper[look]

	if (look == stop.time.gs) 
		{
		stop.gs=1
		if (S.B[look] > upper.B.gs[look]) rej.B.gs=1 
		}
	if (look >= k.tilde) if (S.B[look] > upper.B.gs[look]) 
		{
		stop.gs=1
		rej.B.gs=1 
		stop.time.gs = look
		}
	}

}


if (print==1) 
	{
	print(upper.A)
	print(S.A[1:(k.tilde-1)])
	print(stop.time)

	print(S.B)

	print(rho)
	print(upper.B)
	print(check.diff)
	print(theta.A.star)
	print(stop.A)
	print(rej.B)
	print(stop)
	print(stop.time)

	print(upper.B.naive)
	print(rej.B.naive)
	print(stop.naive)
	print(stop.time.naive)

	print(upper.B.gs)
	print(rej.B.gs)
	print(stop.gs)
	print(stop.time.gs)
	}

data.frame(rej.B,stop.time, rej.B.naive,stop.time.naive, rej.B.gs,stop.time.gs)
}






t = seq(1,5)/5
K = length(t)
alpha.star.u = 0.025*t
alpha.star.l = rep(0,K)

theta.target = 0.5
power = 0.9

boundary = get.boundary(t, alpha.star.u, alpha.star.l, theta.target, power)
boundary

I.max = boundary$I[K]



nsim = 10000
print = 0

k.tilde = 2
theta.B = 0

for (theta.A in c(-0.3,-0.1,0,0.1,0.3,0.5))
	{
	print(theta.A)

	simout.obj = mclapply(seq(1,nsim),simulate.trial,t=t, I.max=I.max, theta.A=theta.A, theta.B=theta.B, k.tilde=k.tilde, print=print, seed=19032025)
	simout.mat = matrix(unlist(simout.obj),ncol=nsim)
	sim.out = as.data.frame(t(simout.mat))
	names(sim.out) = c("rej.B","stop.time","rej.B.naive","stop.time.naive","rej.B.gs","stop.time.gs")
	sim.out[1:10,]

	# print out pr(stop) and pr(stop and reject H_0B) at each look for each boundary (corrected, naive and group-sequential)
	# first row gives pr(no stopping) and overall type I error rate for each boundary
	for (look in seq(0,5))
		{
		tablerow = c(look)
		if (look == 0) for (boundary in seq(1,3)) tablerow = c(tablerow, sum(sim.out[,boundary*2]==0)/nsim, sum(sim.out[,boundary*2-1]==1) / nsim)
		if (look >= 1) for (boundary in seq(1,3)) tablerow = c(tablerow, sum(sim.out[,boundary*2]==look)/nsim, sum((sim.out[,boundary*2-1]==1)*(sim.out[,boundary*2]==look)==1) / nsim)
		print(tablerow)
		}

	}




